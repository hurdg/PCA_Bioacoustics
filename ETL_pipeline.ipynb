{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amphibian Audio Data ETL Notebook\n",
    "\n",
    "### Overview:\n",
    "\n",
    "This notebook serves as a tool for the reformatting of amphibian audio data collected from 2019 through 2023.\n",
    "\n",
    "### Objective:\n",
    "\n",
    "The primary objective of this notebook is to reformat raw audio recordings into an analytically usable format and create a common file structurte. The audio recordings will be clipped into positive and negative samples, as defined by the annual summary reports provided through WIldTrax.\n",
    "\n",
    "### Contents:\n",
    "\n",
    "1. **Data Extraction**:\n",
    "   - Accessing and retrieving historical audio files from internal repositories.\n",
    "   - Reviewing the structure and organization of audio data collected during the specified timeframe.\n",
    "   \n",
    "2. **Data Preprocessing**:\n",
    "   - Conducting thorough cleaning and filtering of historical audio recordings.\n",
    "   - Standardizing audio formats and associated metadata to ensure consistency across datasets.\n",
    "   \n",
    "3. **Feature Extraction**:\n",
    "   - Extracting pertinent features from historical audio signals to aid in subsequent analysis.\n",
    "   - Calculating acoustic metrics essential for amphibian species identification and classification within the designated timeframe.\n",
    "   \n",
    "4. **Data Loading**:\n",
    "   - Efficiently loading processed audio data into structured databases or file systems.\n",
    "   - Establishing robust data pipelines for automated ETL processes, ensuring scalability and repeatability.\n",
    "\n",
    "### For future use :\n",
    "\n",
    "1. Execute each code cell sequentially by pressing Shift + Enter.\n",
    "2. Adhere to the provided instructions and comments within the code cells for guidance throughout the reformatting process.\n",
    "3. Tailor the code and parameters to suit the specific requirements and characteristics of the file structures of the data obtained after 2023.\n",
    "\n",
    "#### Environment Setup:\n",
    "\n",
    "Prior to commencing the reformatting process, ensure the presence of requisite Python libraries and dependencies. It is recommended to employ established tools such as Anaconda or virtual environments to manage the Python environment effectively.\n",
    "__________________________________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py v 3.12.3\n",
    "import os\n",
    "import glob\n",
    "import wave\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "#Not in base python\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify years for data collection\n",
    "year_list = ['2019', '2021', '2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders in parent directory (./analysis/) to store wofr, weto, and negative audio samples (non-weto & non-wofr)\n",
    "\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "\n",
    "wofr_path = os.path.join(parent, 'data', 'positive', 'wofr')\n",
    "if not os.path.exists(wofr_path):\n",
    "    os.makedirs(wofr_path)\n",
    "\n",
    "weto_path = os.path.join(parent, 'data', 'positive', 'weto')\n",
    "if not os.path.exists(weto_path):\n",
    "    os.makedirs(weto_path)\n",
    "\n",
    "negative_path = os.path.join(parent, 'data', 'negative')\n",
    "if not os.path.exists(negative_path):\n",
    "    os.makedirs(negative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect filepaths for annual data - preference shown for copies with naming convention that is consistent with WildTrax uploads\n",
    "\n",
    "wav_root = os.path.join('\\\\\\\\BAN-NAS-DATA', 'EI_Monitoring', 'Amphibian recordings')\n",
    "\n",
    "filepath_wav = [glob.glob(os.path.join(wav_root, '2019', 'Site*', '2019*', '*.wav')),\n",
    "                glob.glob(os.path.join(wav_root, '2021', '*for WildTrax', '2021*', '*.wav')),\n",
    "                glob.glob(os.path.join(wav_root, '2022', '*WildTrax copies', '*.wav')),\n",
    "                glob.glob(os.path.join(wav_root, '2023', '*.wav'))]\n",
    "\n",
    "\n",
    "#Create dictionary with years as the keys and the audio .wav filepaths as values\n",
    "\n",
    "wav_dict = dict(zip(year_list, filepath_wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect metadata (main_report and recording_report CSVs from WIldTrax)\n",
    "\n",
    "metadata_root = os.path.join('\\\\\\\\Ban-files-01', 'groups', 'Resource Conservation', 'EI Monitoring', 'Amphibians', '_ARUs', 'Data and metadata', 'Data from WildTrax')\n",
    "filepath_metadata_tags = glob.glob(os.path.join(metadata_root, '*', '*main_report.csv'))\n",
    "filepath_metadata_index = glob.glob(os.path.join(metadata_root, '*', '*recording_report.csv'))\n",
    "\n",
    "\n",
    "#Create dictionary with years as keys and the 'main_report' .csv filepaths as values\n",
    "\n",
    "metadata_tag_dict = dict(zip(year_list, filepath_metadata_tags))\n",
    "\n",
    "\n",
    "#Create dictionary with years as keys and the 'recording_report' .csv filepaths as values\n",
    "\n",
    "metadata_index_dict = dict(zip(year_list, filepath_metadata_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary of 'recording_id' and 'source_file_name' from main_report.csv and recording_report.csv, respectively.\n",
    "#This step is necessary to index tags from wildtrax back to the original wav files\n",
    "#NOTE: It appears that the source filenames listed in the wildtrax metadata (recording_report.csv) does not always align with existing wav files\n",
    "#       This appears to be true for all of 2019 data\n",
    "\n",
    "fileindex_dict = {}\n",
    "\n",
    "for year in year_list:\n",
    "    #fileindex_dict = {}\n",
    "    with open(metadata_index_dict[year],'r') as f: \n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if row[7] not in fileindex_dict.keys(): #Prevent duplicate entries\n",
    "                fileindex_dict.update({row[7]: row[-2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dict of all possible tag codes and common names that have been used for amphibian recortdings in the wildtrax database\n",
    "\n",
    "tagname_tagcode_dict = {}\n",
    "\n",
    "for year in year_list:\n",
    "    tmp_dict = dict(zip(list(pd.read_csv(metadata_tag_dict[year], usecols = ['species_code'])['species_code']), \n",
    "                        list(pd.read_csv(metadata_tag_dict[year], usecols = ['species_common_name'])['species_common_name'])\n",
    "                        ))\n",
    "    tagname_tagcode_dict = {**tagname_tagcode_dict, **tmp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a nested Function (x3) to accept wav filepath and cut it in accordance with specified start time and clip duration\n",
    "#Resulting file is deposited at outpath\n",
    "\n",
    "def snip_file(filepath, start, duration, outpathfile, min_snip_length = 2):\n",
    "        # file to extract the snippet from\n",
    "    \n",
    "    if duration != duration or start != start:\n",
    "        print(f\"{filepath} contains NAN for start and/or duration\")\n",
    "    else:\n",
    "        with wave.open(filepath, \"rb\") as infile:\n",
    "            # get file data\n",
    "            nchannels = infile.getnchannels()\n",
    "            sampwidth = infile.getsampwidth()\n",
    "            framerate = infile.getframerate()\n",
    "            # extract data\n",
    "            # adjust so that the minimum extraction length is 2s\n",
    "            if duration > 2:\n",
    "                # set position in wave to start of segment\n",
    "                infile.setpos(int(start * framerate))\n",
    "                data = infile.readframes(int(duration * framerate))\n",
    "            else:\n",
    "                # Set position in wave to centre - (min_snip_length/2)\n",
    "                start_adj = start + (duration/2) - (min_snip_length/2)\n",
    "                # Set start time floor at 0s\n",
    "                if start_adj < 0:\n",
    "                    start_adj = 0\n",
    "                infile.setpos(int((start_adj) * framerate))\n",
    "                data = infile.readframes(int(min_snip_length * framerate))\n",
    "\n",
    "        # write the extracted data to a new file\n",
    "        with wave.open(outpathfile, 'w') as outfile:\n",
    "            outfile.setnchannels(nchannels)\n",
    "            outfile.setsampwidth(sampwidth)\n",
    "            outfile.setframerate(framerate)\n",
    "            outfile.setnframes(int(len(data) / sampwidth))\n",
    "            outfile.writeframes(data)\n",
    "\n",
    "\n",
    "#Row-wise function to extract year of the row in question. ALong with start time of a proposed audio clip tag, and the duration. THe filepath is also identified.\n",
    "\n",
    "def snip_row(df_row, index, file_dict, outpath):\n",
    "    datetime = df_row['recording_date_time']\n",
    "    year = str(datetime.year)\n",
    "    start = df_row['detection_time']\n",
    "    duration = df_row['tag_duration']\n",
    "    recording_id = df_row['recording_id']\n",
    "    filename = index[recording_id]\n",
    "    matching_filepaths = [filepath for filepath in file_dict[year] if filename in filepath]\n",
    "    if len(matching_filepaths) >0:\n",
    "        filepath = matching_filepaths[0]\n",
    "        outpathfile = outpath + \"/\" + filename[:-4] + '_' + str(start) + '_' + str(duration) + '.wav'\n",
    "        snip_file(filepath = filepath, start = start, duration = duration, outpathfile = outpathfile)\n",
    "    else: \n",
    "        print(f\"No matching wave file for {filename}\")\n",
    "\n",
    "\n",
    "#Wrapper to accept csv filepath as input and process with above functions\n",
    "\n",
    "def snip_csv(wildtrax_mainreport_filepath, species_code, index, outpath, file_dict, regex= False):\n",
    "    df = pd.read_csv(wildtrax_mainreport_filepath, parse_dates = ['recording_date_time'], dtype = {'recording_id': str})\n",
    "    species_types = list(tagname_tagcode_dict.keys())\n",
    "    if species_code.casefold() not in [str.casefold(accepted_code) for accepted_code in species_types] and regex == False:\n",
    "        raise ValueError(f\"Invalid species_code. Expected one of: %s {species_types}\")\n",
    "    species_mask = df['species_code'].str.contains(species_code, case = False, regex = regex)\n",
    "    df[species_mask].apply(snip_row, index = index, outpath = outpath, file_dict = file_dict, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate folder with snipped audio samples for weto and wofr\n",
    "\n",
    "for year in year_list:\n",
    "    snip_csv(metadata_tag_dict[year], species_code = 'WETO', index = fileindex_dict , outpath = weto_path, file_dict = wav_dict)\n",
    "    snip_csv(metadata_tag_dict[year], species_code = 'WOFR', index = fileindex_dict , outpath = wofr_path, file_dict = wav_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the tag codes and corresponding names\n",
    "\n",
    "for code in sorted(tagname_tagcode_dict): print(code, \":\", tagname_tagcode_dict[code]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# NOTE: It was found that all samples with tags that are NOT weto or wofr still may ocntain weto and wofr vocalizations.\n",
    "#       Therefore, the clips corresponding to these tags may not be used as negative samples\n",
    "\n",
    "\n",
    "\n",
    "#Identify all possible species_tags that do not correspond to a potential target vocalizations (i.e., weto or wofr)\n",
    "#Additionally remove all tags that may correspond to background noise, as frog vocalizations may still be present in these clips\n",
    "\n",
    "#potential_target = ['WETO', 'WOFR', 'UNFR', 'UNKN']\n",
    "\n",
    "#negative_tag_list = [species_tag for species_tag in list(tagname_tagcode_dict.keys()) if species_tag not in \n",
    "#                    potential_target]\n",
    "\n",
    "\n",
    "#Reformat list to regex compatible string (i.e., a '|' delineated string)\n",
    "\n",
    "#negative_regex = \"|\".join(map(str, negative_tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for year in year_list:\n",
    "#    snip_csv(metadata_tag_dict[year], species_code = negative_regex, index = fileindex_dict , outpath = negative_path, file_dict = wav_dict, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New strategy to populate negative folder with random samples from recordings classified with absence of target species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders in negative directory (./analysis/negative) to store none_wofr, none_weto, and all_negative audio samples\n",
    "\n",
    "all_negative_path = os.path.join(parent, 'data', 'negative', 'none')\n",
    "if not os.path.exists(all_negative_path):\n",
    "    os.makedirs(all_negative_path)\n",
    "\n",
    "weto_negative_path = os.path.join(parent, 'data', 'negative', 'no_weto')\n",
    "if not os.path.exists(weto_negative_path):\n",
    "    os.makedirs(weto_negative_path)\n",
    "\n",
    "wofr_negative_path = os.path.join(parent, 'data', 'negative', 'no_wofr')\n",
    "if not os.path.exists(wofr_negative_path):\n",
    "    os.makedirs(wofr_negative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to identify recording_ids that do not contain any tags of specified species\n",
    "\n",
    "def get_neg_recordIDs(neg_species:str, csv_filepath:str):\n",
    "    df = pd.read_csv(csv_filepath, usecols = ['recording_id', 'species_code'])\n",
    "    neg_mask = df.groupby(['recording_id'])['species_code'].apply(lambda x: any(x == neg_species))\n",
    "    neg_dict = dict(zip(neg_mask.index, neg_mask.values))\n",
    "    return(neg_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function to all recordings with regard to wofr and weto\n",
    "\n",
    "no_weto_dict = {}\n",
    "no_wofr_dict = {}\n",
    "\n",
    "\n",
    "for year in year_list:\n",
    "    no_weto = get_neg_recordIDs('WETO', metadata_tag_dict[year])\n",
    "    no_wofr = get_neg_recordIDs('WOFR', metadata_tag_dict[year])\n",
    "    no_weto_dict = {**no_weto_dict, **no_weto}\n",
    "    no_wofr_dict = {**no_wofr_dict, **no_wofr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to subset the main_report csvs from wildtrax to only include recordings that DO NOT contain the specified species code tag\n",
    "\n",
    "def get_neg_filepaths(neg_species:str, main_report_filepath:str, file_dict):\n",
    "    df = pd.read_csv(main_report_filepath, parse_dates = ['recording_date_time'], dtype = {'recording_id': str}) #read in csv\n",
    "    neg_mask = df[['recording_id', 'species_code']].groupby(['recording_id'])['species_code'].apply(lambda x: any(x == neg_species))\n",
    "    neg_dict = dict(zip(neg_mask.index, neg_mask.values)) #dict of key= recording_id and value= target species presence\n",
    "    neg_record_ids = [record_id for record_id, presence in neg_dict.items() if not presence] #extract record_ids of audio files that do not contain target species\n",
    "    non_null_mask = (df['species_code'] != 'NaN') & (df['species_code'] != 'NONE') & (df['recording_id'].isin(neg_record_ids)) #create maske\n",
    "    masked_df = df[non_null_mask] #apply mask\n",
    "    return(masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_210000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190508_230000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_000000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190509_010000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_170000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-11_20190505_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_160000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_170000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_180000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_190000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for BANFF-A-147_20190514_200000.wav\n",
      "No matching wave file for A-11-E_20210506_070000.wav\n",
      "No matching wave file for A-11-E_20210506_130000.wav\n",
      "No matching wave file for A-11-S_20210505_140000.wav\n",
      "No matching wave file for A-11-S_20210505_150000.wav\n",
      "No matching wave file for A-11-S_20210505_160000.wav\n",
      "No matching wave file for A-120-E_20210504_200000.wav\n",
      "No matching wave file for A-120-E_20210504_210000.wav\n",
      "No matching wave file for A-120-E_20210504_220000.wav\n",
      "No matching wave file for A-142-E_20210509_010000.wav\n",
      "No matching wave file for A-142-E_20210509_040000.wav\n",
      "No matching wave file for A-142-E_20210509_160000.wav\n",
      "No matching wave file for A-142-E_20210509_190000.wav\n",
      "No matching wave file for A-142-E_20210509_200000.wav\n",
      "No matching wave file for A-142-E_20210509_220000.wav\n",
      "No matching wave file for A-142-E_20210509_230000.wav\n",
      "No matching wave file for A-158-N_20210507_010000.wav\n",
      "No matching wave file for A-158-S_20210504_210000.wav\n",
      "No matching wave file for A-17-E_20210507_130000.wav\n",
      "No matching wave file for A-17-E_20210507_160000.wav\n",
      "No matching wave file for A-17-E_20210507_190000.wav\n",
      "No matching wave file for A-17-E_20210507_200000.wav\n",
      "No matching wave file for A-17-E_20210507_210000.wav\n",
      "No matching wave file for A-17-E_20210507_220000.wav\n",
      "No matching wave file for A-17-W_20210510_130000.wav\n",
      "No matching wave file for A-17-W_20210510_160000.wav\n",
      "No matching wave file for A-17-W_20210510_190000.wav\n",
      "No matching wave file for A-17-W_20210510_200000.wav\n",
      "No matching wave file for A-17-W_20210510_220000.wav\n",
      "No matching wave file for A-206-N_20210417_014000.wav\n",
      "No matching wave file for A-40_20210509_010000.wav\n",
      "No matching wave file for A-47_20210506_190000.wav\n",
      "No matching wave file for A-47_20210506_200000.wav\n",
      "No matching wave file for A-47_20210506_210000.wav\n",
      "No matching wave file for A-47_20210506_220000.wav\n",
      "No matching wave file for A-60-E_20210507_010000.wav\n",
      "No matching wave file for A-60-E_20210507_040000.wav\n",
      "No matching wave file for A-60-E_20210507_160000.wav\n",
      "No matching wave file for A-60-E_20210507_210000.wav\n",
      "No matching wave file for A-60-W_20210508_010000.wav\n",
      "No matching wave file for A-60-W_20210508_220000.wav\n",
      "No matching wave file for A-60-W_20210508_230000.wav\n",
      "No matching wave file for A-71-S_20210508_010000.wav\n",
      "No matching wave file for A-71-S_20210508_040000.wav\n",
      "No matching wave file for A-74_20210505_010000.wav\n",
      "No matching wave file for A-74_20210505_040000.wav\n",
      "No matching wave file for A-74_20210505_130000.wav\n",
      "No matching wave file for A-74_20210505_160000.wav\n",
      "No matching wave file for A-74_20210505_190000.wav\n",
      "No matching wave file for A-74_20210505_200000.wav\n",
      "No matching wave file for A-74_20210505_210000.wav\n",
      "No matching wave file for A-74_20210505_220000.wav\n",
      "No matching wave file for A-11-E_20210506_130000.wav\n",
      "No matching wave file for A-17-E_20210507_010000.wav\n",
      "No matching wave file for A-17-E_20210507_010000.wav\n",
      "No matching wave file for A-206-N_20210417_014000.wav\n",
      "No matching wave file for A-71-S_20210508_210000.wav\n",
      "No matching wave file for A-9_20230509_225500.wav\n",
      "No matching wave file for A-9_20230509_225500.wav\n",
      "No matching wave file for A-9_20230509_225500.wav\n",
      "No matching wave file for A-9_20230509_225500.wav\n",
      "No matching wave file for A-9_20230510_225500.wav\n",
      "No matching wave file for A-9_20230510_225500.wav\n",
      "No matching wave file for A-9_20230510_225500.wav\n",
      "No matching wave file for A-9_20230520_225500.wav\n",
      "No matching wave file for A-9_20230520_225500.wav\n",
      "No matching wave file for A-9_20230520_225500.wav\n"
     ]
    }
   ],
   "source": [
    "#Identify and extract audio clips that DO NOT contain the specified target species, but DO contain non-target species\n",
    "\n",
    "for year in year_list:\n",
    "    #Apply function to create df that does not contain target species, but does contain tagged vocalizations/sounds\n",
    "    weto_masked_df = get_neg_filepaths(neg_species ='WETO', main_report_filepath = metadata_tag_dict[year], file_dict = wav_dict[year])\n",
    "    wofr_masked_df = get_neg_filepaths(neg_species ='WOFR', main_report_filepath = metadata_tag_dict[year], file_dict = wav_dict[year])\n",
    "\n",
    "    #apply snip_row function (created above) to populate negative folders\n",
    "    weto_masked_df.apply(snip_row, index = fileindex_dict, outpath = weto_negative_path, file_dict = wav_dict, axis = 1)\n",
    "    wofr_masked_df.apply(snip_row, index = fileindex_dict, outpath = wofr_negative_path, file_dict = wav_dict, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
